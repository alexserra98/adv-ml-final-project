{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from functools import *\n",
    "import gdown\n",
    "from typing import List, Tuple, Union, Optional\n",
    "from fancy_einsum import einsum\n",
    "import einops\n",
    "from jaxtyping import Float, Int\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformer_lens import utils, ActivationCache, HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.components import LayerNorm\n",
    "from src.circuit import *\n",
    "from src.fourier import *\n",
    "from src.my_utils import *\n",
    "from src.train import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ('Grokking' / 'saved_runs').resolve()\n",
    "large_root = ('Grokking' / 'large_files').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 113\n",
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 1,\n",
    "    d_vocab = p+1,\n",
    "    d_model = 128,\n",
    "    d_mlp = 4 * 128,\n",
    "    n_heads = 4,\n",
    "    d_head = 128 // 4,\n",
    "    n_ctx = 3,\n",
    "    act_fn = \"relu\",\n",
    "    normalization_type = None,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "model = HookedTransformer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not large_root.exists():\n",
    "    !git clone https://github.com/neelnanda-io/Grokking.git\n",
    "    os.mkdir(large_root)\n",
    "\n",
    "full_run_data_path = (large_root / \"full_run_data.pth\").resolve()\n",
    "if not full_run_data_path.exists():\n",
    "    url = \"https://drive.google.com/uc?id=12pmgxpTHLDzSNMbMCuAMXP1lE_XiCQRy\"\n",
    "    output = str(full_run_data_path)\n",
    "    gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run_data = t.load(full_run_data_path)\n",
    "state_dict = full_run_data[\"state_dicts\"][400]\n",
    "\n",
    "model = load_in_state_dict(model, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper variables\n",
    "W_O = model.W_O[0]\n",
    "W_K = model.W_K[0]\n",
    "W_Q = model.W_Q[0]\n",
    "W_V = model.W_V[0]\n",
    "W_in = model.W_in[0]\n",
    "W_out = model.W_out[0]\n",
    "W_pos = model.W_pos\n",
    "W_E = model.W_E[:-1]\n",
    "final_pos_resid_initial = model.W_E[-1] + W_pos[2]\n",
    "W_U = model.W_U[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = t.tensor([(i, j, p) for i in range(p) for j in range(p)]).to(device)\n",
    "labels = t.tensor([fn(i, j) for i, j, _ in all_data]).to(device)\n",
    "original_logits, cache = model.run_with_cache(all_data)\n",
    "# Final position only, also remove the logits for `=`\n",
    "original_logits = original_logits[:, -1, :-1]\n",
    "original_loss = cross_entropy_high_precision(original_logits, labels)\n",
    "print(f\"Original loss: {original_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_logit = W_out @ W_U\n",
    "\n",
    "W_OV = W_V @ W_O\n",
    "W_neur = W_E @ W_OV @ W_in\n",
    "\n",
    "W_QK = W_Q @ W_K.transpose(-1, -2)\n",
    "W_attn = final_pos_resid_initial @ W_QK @ W_E.T / (cfg.d_head ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mat = cache['pattern', 0][:, :, 2]\n",
    "neuron_acts_post = cache['post', 0][:, -1]\n",
    "neuron_acts_pre = cache['pre', 0][:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Periodicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mat = attn_mat[:, :, :2]\n",
    "# Note, we ignore attn from 2 -> 2\n",
    "\n",
    "attn_mat_sq = einops.rearrange(attn_mat, \"(x y) head seq -> x y head seq\", x=p)\n",
    "# We rearranged attn_mat, so the first two dims represent (x, y) in modular arithmetic equation\n",
    "\n",
    "inputs_heatmap(\n",
    "    attn_mat_sq[..., 0],\n",
    "    title=f'Attention score for heads at position 0',\n",
    "    animation_frame=2,\n",
    "    animation_name='head'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neuron Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_acts_post_sq = einops.rearrange(neuron_acts_post, \"(x y) d_mlp -> x y d_mlp\", x=p)\n",
    "neuron_acts_pre_sq = einops.rearrange(neuron_acts_pre, \"(x y) d_mlp -> x y d_mlp\", x=p)\n",
    "# We rearranged activations, so the first two dims represent (x, y) in modular arithmetic equation\n",
    "\n",
    "top_k = 3\n",
    "inputs_heatmap(\n",
    "    neuron_acts_post_sq[..., :top_k],\n",
    "    title=f'Activations for first {top_k} neurons',\n",
    "    animation_frame=2,\n",
    "    animation_name='Neuron'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Effective weights:**\n",
    "\n",
    "**$W_{neur}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "animate_multi_lines(\n",
    "    W_neur[..., :top_k],\n",
    "    y_index = [f'head {hi}' for hi in range(4)],\n",
    "    labels = {'x':'Input token', 'value':'Contribution to neuron'},\n",
    "    snapshot='Neuron',\n",
    "    title=f'Contribution to first {top_k} neurons via OV-circuit of heads (not weighted by attention)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$W_{attn}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines(\n",
    "    W_attn,\n",
    "    labels = [f'head {hi}' for hi in range(4)],\n",
    "    xaxis='Input token',\n",
    "    yaxis='Contribution to attn score',\n",
    "    title=f'Contribution to attention score (pre-softmax) for each head'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_basis, fourier_basis_names = make_fourier_basis(p)\n",
    "\n",
    "animate_lines(\n",
    "    fourier_basis,\n",
    "    snapshot_index=fourier_basis_names,\n",
    "    snapshot='Fourier Component',\n",
    "    title='Graphs of Fourier Components (Use Slider)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(fourier_basis @ fourier_basis.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations in fourier space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Fourier transformation\n",
    "attn_mat_fourier_basis = fft2d(attn_mat_sq, fourier_basis)\n",
    "\n",
    "# Plot results\n",
    "imshow_fourier(\n",
    "    attn_mat_fourier_basis[..., 0],\n",
    "    title=f'Attention score for heads at position 0, in Fourier basis',\n",
    "    animation_frame=2,\n",
    "    animation_name='head'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neuron Activations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_acts_post_fourier_basis = fft2d(neuron_acts_post_sq, fourier_basis)\n",
    "\n",
    "top_k = 3\n",
    "imshow_fourier(\n",
    "    neuron_acts_post_fourier_basis[..., :top_k],\n",
    "    title=f'Activations for first {top_k} neurons',\n",
    "    animation_frame=2,\n",
    "    animation_name='Neuron'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_neur_fourier = fft1d_given_dim(W_neur, dim=1)\n",
    "\n",
    "top_k = 5\n",
    "animate_multi_lines(\n",
    "    W_neur_fourier[..., :top_k],\n",
    "    y_index = [f'head {hi}' for hi in range(4)],\n",
    "    labels = {'x':'Fourier component', 'value':'Contribution to neuron'},\n",
    "    snapshot='Neuron',\n",
    "    hover=fourier_basis_names,\n",
    "    title=f'Contribution to first {top_k} neurons via OV-circuit of heads (not weighted by attn), in Fourier basis'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines(\n",
    "    fft1d(W_attn),\n",
    "    labels = [f'head {hi}' for hi in range(4)],\n",
    "    xaxis='Input token',\n",
    "    yaxis = 'Contribution to attn score',\n",
    "    title=f'Contribution to attn score (pre-softmax) for each head, in Fourier Basis',\n",
    "    hover=fourier_basis_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(\n",
    "    (fourier_basis @ W_E).pow(2).sum(1),\n",
    "    hover=fourier_basis_names,\n",
    "    title='Norm of embedding of each Fourier Component',\n",
    "    xaxis='Fourier Component',\n",
    "    yaxis='Norm'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_acts_centered = neuron_acts_post_sq - neuron_acts_post_sq.mean((0, 1), keepdim=True)\n",
    "\n",
    "# Take 2D Fourier transform\n",
    "neuron_acts_centered_fourier = fft2d(neuron_acts_centered, fourier_basis)\n",
    "\n",
    "\n",
    "imshow_fourier(\n",
    "    neuron_acts_centered_fourier.pow(2).mean(-1),\n",
    "    title=f\"Norms of 2D Fourier components of centered neuron activations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_freqs, neuron_frac_explained = find_neuron_freqs(neuron_acts_centered_fourier)\n",
    "key_freqs, neuron_freq_counts = t.unique(neuron_freqs, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_of_activations_positive_at_posn2 = (cache['pre', 0][:, -1] > 0).float().mean(0)\n",
    "\n",
    "scatter(\n",
    "    x=neuron_freqs,\n",
    "    y=neuron_frac_explained,\n",
    "    xaxis=\"Neuron frequency\",\n",
    "    yaxis=\"Frac explained\",\n",
    "    colorbar_title=\"Frac positive\",\n",
    "    title=\"Fraction of neuron activations explained by key freq\",\n",
    "    color=utils.to_numpy(fraction_of_activations_positive_at_posn2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To represent that they are in a special sixth cluster, we set the frequency of these neurons to -1\n",
    "neuron_freqs[neuron_frac_explained < 0.85] = -1.\n",
    "key_freqs_plus = t.concatenate([key_freqs, -key_freqs.new_ones((1,))])\n",
    "\n",
    "for i, k in enumerate(key_freqs_plus):\n",
    "    print(f'Cluster {i}: freq k={k}, {(neuron_freqs==k).sum()} neurons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_norms_in_each_cluster = []\n",
    "for freq in key_freqs:\n",
    "    fourier_norms_in_each_cluster.append(\n",
    "        einops.reduce(\n",
    "            neuron_acts_centered_fourier.pow(2)[..., neuron_freqs==freq],\n",
    "            'batch_y batch_x neuron -> batch_y batch_x',\n",
    "            'mean'\n",
    "        )\n",
    "    )\n",
    "\n",
    "imshow_fourier(\n",
    "    t.stack(fourier_norms_in_each_cluster),\n",
    "    title=f'Norm of 2D Fourier components of neuron activations in each cluster',\n",
    "    facet_col=0,\n",
    "    facet_labels=[f\"Freq={freq}\" for freq in key_freqs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_in_freqs = []\n",
    "\n",
    "for freq in key_freqs:\n",
    "\n",
    "    # Get all neuron activations corresponding to this frequency\n",
    "    filtered_neuron_acts = neuron_acts_post[:, neuron_freqs==freq]\n",
    "\n",
    "    # Project onto const/linear/quadratic terms in 2D Fourier basis\n",
    "    filtered_neuron_acts_in_freq = project_onto_frequency(filtered_neuron_acts, freq)\n",
    "\n",
    "    # Calcluate new logits, from these filtered neuron activations\n",
    "    logits_in_freq = filtered_neuron_acts_in_freq @ W_logit[neuron_freqs==freq]\n",
    "\n",
    "    logits_in_freqs.append(logits_in_freq)\n",
    "\n",
    "# We add on neurons in the always firing cluster, unfiltered\n",
    "logits_always_firing = neuron_acts_post[:, neuron_freqs==-1] @ W_logit[neuron_freqs==-1]\n",
    "logits_in_freqs.append(logits_always_firing)\n",
    "\n",
    "# Print new losses\n",
    "print('Loss with neuron activations ONLY in key freq (inclusing always firing cluster)\\n{:.6e}\\n'.format(\n",
    "    test_logits(\n",
    "        sum(logits_in_freqs),\n",
    "        bias_correction=True,\n",
    "        original_logits=original_logits\n",
    "    )\n",
    "))\n",
    "print('Loss with neuron activations ONLY in key freq (exclusing always firing cluster)\\n{:.6e}\\n'.format(\n",
    "    test_logits(\n",
    "        sum(logits_in_freqs[:-1]),\n",
    "        bias_correction=True,\n",
    "        original_logits=original_logits\n",
    "    )\n",
    "))\n",
    "print('Original loss\\n{:.6e}'.format(original_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loss with neuron activations excluding none:     {:.9f}'.format(original_loss.item()))\n",
    "for c, freq in enumerate(key_freqs_plus):\n",
    "    print('Loss with neuron activations excluding freq={}:  {:.9f}'.format(\n",
    "        freq,\n",
    "        test_logits(\n",
    "            sum(logits_in_freqs) - logits_in_freqs[c],\n",
    "            bias_correction=True,\n",
    "            original_logits=original_logits\n",
    "        )\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logits in Fourier Basis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_fourier(\n",
    "    einops.reduce(neuron_acts_centered_fourier.pow(2), 'y x neuron -> y x', 'mean'),\n",
    "    title='Norm of Fourier Components of Neuron Acts'\n",
    ")\n",
    "\n",
    "# Rearrange logits, so the first two dims represent (x, y) in modular arithmetic equation\n",
    "original_logits_sq = einops.rearrange(original_logits, \"(x y) z -> x y z\", x=p)\n",
    "original_logits_fourier = fft2d(original_logits_sq)\n",
    "\n",
    "imshow_fourier(\n",
    "    einops.reduce(original_logits_fourier.pow(2), 'y x z -> y x', 'mean'),\n",
    "    title='Norm of Fourier Components of Logits'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trig_logits = []\n",
    "\n",
    "for k in key_freqs:\n",
    "\n",
    "    cos_xplusy_direction, sin_xplusy_direction = get_trig_sum_directions(k)\n",
    "\n",
    "    cos_xplusy_projection = project_onto_direction(\n",
    "        original_logits,\n",
    "        cos_xplusy_direction.flatten()\n",
    "    )\n",
    "\n",
    "    sin_xplusy_projection = project_onto_direction(\n",
    "        original_logits,\n",
    "        sin_xplusy_direction.flatten()\n",
    "    )\n",
    "\n",
    "    trig_logits.extend([cos_xplusy_projection, sin_xplusy_projection])\n",
    "\n",
    "trig_logits = sum(trig_logits)\n",
    "\n",
    "print(f'Loss with just x+y components: {test_logits(trig_logits, True, original_logits):.4e}')\n",
    "print(f\"Original Loss: {original_loss:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $W_{logits}$ and SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US = W_logit @ fourier_basis.T\n",
    "\n",
    "imshow_div(\n",
    "    US,\n",
    "    x=fourier_basis_names,\n",
    "    yaxis='Neuron index',\n",
    "    title='W_logit in the Fourier Basis',\n",
    "    height=800,\n",
    "    width=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_sorted = t.concatenate([\n",
    "    US[neuron_freqs==freq] for freq in key_freqs_plus\n",
    "])\n",
    "hline_positions = np.cumsum([(neuron_freqs == freq).sum().item() for freq in key_freqs]).tolist() + [cfg.d_mlp]\n",
    "\n",
    "imshow_div(\n",
    "    US_sorted,\n",
    "    x=fourier_basis_names,\n",
    "    yaxis='Neuron',\n",
    "    title='W_logit in the Fourier Basis (rearranged by neuron cluster)',\n",
    "    hline_positions = hline_positions,\n",
    "    hline_labels = [f\"Cluster: {freq=}\" for freq in key_freqs.tolist()] + [\"No freq\"],\n",
    "    height=800,\n",
    "    width=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_components = []\n",
    "sin_components = []\n",
    "\n",
    "for k in key_freqs:\n",
    "    ﾏブ_sin = US[:, 2*k]\n",
    "    ﾏブ_cos = US[:, 2*k-1]\n",
    "\n",
    "    logits_in_cos_dir = neuron_acts_post_sq @ ﾏブ_cos\n",
    "    logits_in_sin_dir = neuron_acts_post_sq @ ﾏブ_sin\n",
    "\n",
    "    cos_components.append(fft2d(logits_in_cos_dir))\n",
    "    sin_components.append(fft2d(logits_in_sin_dir))\n",
    "\n",
    "for title, components in zip(['Cosine', 'Sine'], [cos_components, sin_components]):\n",
    "    imshow_fourier(\n",
    "        t.stack(components),\n",
    "        title=f'{title} components of neuron activations in Fourier basis',\n",
    "        animation_frame=0,\n",
    "        animation_name=\"Frequency\",\n",
    "        animation_labels=key_freqs.tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anaysis during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = full_run_data['epochs']\n",
    "\n",
    "# Define a dictionary to store our metrics in\n",
    "metric_cache = {}\n",
    "plot_metric = partial(lines, x=epochs, xaxis='Epoch', log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_loss = partial(excl_loss, key_freqs=key_freqs)\n",
    "get_metrics(model, metric_cache, excl_loss, 'excl_loss')\n",
    "\n",
    "lines(\n",
    "    t.concat([\n",
    "        metric_cache['excl_loss'].T,\n",
    "        metric_cache['train_loss'][None, :],\n",
    "        metric_cache['test_loss'][None, :]\n",
    "    ], axis=0),\n",
    "    labels=[f'excl {freq}' for freq in key_freqs]+['train', 'test'],\n",
    "    title='Excluded Loss for each trig component',\n",
    "    log_y=True,\n",
    "    x=full_run_data['epochs'],\n",
    "    xaxis='Epoch',\n",
    "    yaxis='Loss'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding in Fourier Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot every 200 epochs so it's not overwhelming\n",
    "get_metrics(model, metric_cache, fourier_embed, 'fourier_embed')\n",
    "\n",
    "animate_lines(\n",
    "    metric_cache['fourier_embed'][::2],\n",
    "    snapshot_index = epochs[::2],\n",
    "    snapshot='Epoch',\n",
    "    hover=fourier_basis_names,\n",
    "    animation_group='x',\n",
    "    title='Norm of Fourier Components in the Embedding Over Training',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model, metric_cache, embed_SVD, 'embed_SVD')\n",
    "\n",
    "animate_lines(\n",
    "    metric_cache['embed_SVD'],\n",
    "    snapshot_index = epochs,\n",
    "    snapshot='Epoch',\n",
    "    title='Singular Values of the Embedding During Training',\n",
    "    xaxis='Singular Number',\n",
    "    yaxis='Singular Value',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Development of Trig Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in ['neuron_pre', 'neuron_post', 'logit']:\n",
    "    get_metrics(\n",
    "        model,\n",
    "        metric_cache,\n",
    "        partial(tensor_trig_ratio, mode=mode),\n",
    "        f\"{mode}_trig_ratio\",\n",
    "        reset=True\n",
    "    )\n",
    "\n",
    "lines_list = []\n",
    "line_labels = []\n",
    "for mode in ['neuron_pre', 'neuron_post', 'logit']:\n",
    "    tensor = metric_cache[f\"{mode}_trig_ratio\"]\n",
    "    lines_list.append(einops.reduce(tensor, 'epoch index -> epoch', 'mean'))\n",
    "    line_labels.append(f\"{mode}_trig_frac\")\n",
    "\n",
    "plot_metric(\n",
    "    lines_list,\n",
    "    labels=line_labels,\n",
    "    log_y=False,\n",
    "    yaxis='Ratio',\n",
    "    title='Fraction of logits and neurons explained by trig terms',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development of neuron activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frac_explained(model: HookedTransformer):\n",
    "    _, cache = model.run_with_cache(all_data, return_type=None)\n",
    "\n",
    "    returns = []\n",
    "\n",
    "    for neuron_type in ['pre', 'post']:\n",
    "        neuron_acts = cache[neuron_type, 0][:, -1].clone().detach()\n",
    "        neuron_acts_centered = neuron_acts - neuron_acts.mean(0)\n",
    "        neuron_acts_fourier = fft2d(\n",
    "            einops.rearrange(neuron_acts_centered, \"(x y) neuron -> x y neuron\", x=p)\n",
    "        )\n",
    "\n",
    "        # Calculate the sum of squares over all inputs, for each neuron\n",
    "        square_of_all_terms = einops.reduce(\n",
    "            neuron_acts_fourier.pow(2), \"x y neuron -> neuron\", \"sum\"\n",
    "        )\n",
    "\n",
    "        frac_explained = t.zeros(d_mlp).to(device)\n",
    "        frac_explained_quadratic_terms = t.zeros(d_mlp).to(device)\n",
    "\n",
    "        for freq in key_freqs_plus:\n",
    "            # Get Fourier activations for neurons in this frequency cluster\n",
    "            # We arrange by frequency (i.e. each freq has a 3x3 grid with const, linear & quadratic terms)\n",
    "            acts_fourier = arrange_by_2d_freqs(neuron_acts_fourier[..., neuron_freqs==freq])\n",
    "\n",
    "            # Calculate the sum of squares over all inputs, after filtering for just this frequency\n",
    "            # Also calculate the sum of squares for just the quadratic terms in this frequency\n",
    "            if freq==-1:\n",
    "                squares_for_this_freq = squares_for_this_freq_quadratic_terms = einops.reduce(\n",
    "                    acts_fourier[:, 1:, 1:].pow(2), \"freq x y neuron -> neuron\", \"sum\"\n",
    "                )\n",
    "            else:\n",
    "                squares_for_this_freq = einops.reduce(\n",
    "                    acts_fourier[freq-1].pow(2), \"x y neuron -> neuron\", \"sum\"\n",
    "                )\n",
    "                squares_for_this_freq_quadratic_terms = einops.reduce(\n",
    "                    acts_fourier[freq-1, 1:, 1:].pow(2), \"x y neuron -> neuron\", \"sum\"\n",
    "                )\n",
    "\n",
    "            frac_explained[neuron_freqs==freq] = squares_for_this_freq / square_of_all_terms[neuron_freqs==freq]\n",
    "            frac_explained_quadratic_terms[neuron_freqs==freq] = squares_for_this_freq_quadratic_terms / square_of_all_terms[neuron_freqs==freq]\n",
    "\n",
    "        returns.extend([frac_explained, frac_explained_quadratic_terms])\n",
    "\n",
    "    frac_active = (neuron_acts > 0).float().mean(0)\n",
    "\n",
    "    return t.nan_to_num(t.stack(returns + [neuron_freqs, frac_active], axis=0))\n",
    "\n",
    "\n",
    "get_metrics(model, metric_cache, get_frac_explained, 'get_frac_explained')\n",
    "\n",
    "frac_explained_pre = metric_cache['get_frac_explained'][:, 0]\n",
    "frac_explained_quadratic_pre = metric_cache['get_frac_explained'][:, 1]\n",
    "frac_explained_post = metric_cache['get_frac_explained'][:, 2]\n",
    "frac_explained_quadratic_post = metric_cache['get_frac_explained'][:, 3]\n",
    "neuron_freqs_ = metric_cache['get_frac_explained'][:, 4]\n",
    "frac_active = metric_cache['get_frac_explained'][:, 5]\n",
    "\n",
    "animate_scatter(\n",
    "    t.stack([frac_explained_quadratic_pre, frac_explained_quadratic_post], dim=1)[:200:5],\n",
    "    color=neuron_freqs_[:200:5],\n",
    "    color_name='freq',\n",
    "    snapshot='epoch',\n",
    "    snapshot_index=epochs[:200:5],\n",
    "    xaxis='Quad ratio pre',\n",
    "    yaxis='Quad ratio post',\n",
    "    color_continuous_scale='viridis',\n",
    "    title='Fraction of variance explained by quadratic terms (up to epoch 20K)'\n",
    ")\n",
    "\n",
    "animate_scatter(\n",
    "    t.stack([neuron_freqs_, frac_explained_pre, frac_explained_post], dim=1)[:200:5],\n",
    "    color=frac_active[:200:5],\n",
    "    color_name='frac_active',\n",
    "    snapshot='epoch',\n",
    "    snapshot_index=epochs[:200:5],\n",
    "    xaxis='Freq',\n",
    "    yaxis='Frac explained',\n",
    "    hover=list(range(d_mlp)),\n",
    "    color_continuous_scale='viridis',\n",
    "    title='Fraction of variance explained by this frequency (up to epoch 20K)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development of commutativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model, metric_cache, avg_attn_pattern, 'avg_attn_pattern')\n",
    "\n",
    "imshow_div(\n",
    "    metric_cache['avg_attn_pattern'][::5],\n",
    "    animation_frame=0,\n",
    "    animation_name='head',\n",
    "    title='Avg attn by position and head, snapped every 100 epochs',\n",
    "    xaxis='Pos',\n",
    "    yaxis='Head',\n",
    "    zmax=0.5,\n",
    "    zmin=0.0,\n",
    "    color_continuous_scale='Blues',\n",
    "    text_auto='.3f',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines(\n",
    "    (metric_cache['avg_attn_pattern'][:, :, 0]-metric_cache['avg_attn_pattern'][:, :, 1]).T,\n",
    "    labels=[f\"head {i}\" for i in range(4)],\n",
    "    x=epochs,\n",
    "    xaxis='Epoch',\n",
    "    yaxis='Average difference',\n",
    "    title='Attention to pos 0 - pos 1 by head over training'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model, metric_cache, trig_loss, 'trig_loss')\n",
    "\n",
    "trig_loss_train = partial(trig_loss, mode='train')\n",
    "get_metrics(model, metric_cache, trig_loss_train, 'trig_loss_train')\n",
    "\n",
    "line_labels = ['test_loss', 'train_loss', 'trig_loss', 'trig_loss_train']\n",
    "plot_metric([metric_cache[lab] for lab in line_labels], labels=line_labels, title='Different losses over training')\n",
    "plot_metric([metric_cache['test_loss']/metric_cache['trig_loss']], title='Ratio of trig and test loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development of squared sum of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_names = [name for name, param in model.named_parameters()]\n",
    "\n",
    "def sum_sq_weights(model):\n",
    "    return [param.pow(2).sum().item() for name, param in model.named_parameters()]\n",
    "get_metrics(model, metric_cache, sum_sq_weights, 'sum_sq_weights')\n",
    "\n",
    "plot_metric(\n",
    "    metric_cache['sum_sq_weights'].T,\n",
    "    title='Sum of squared weights for each parameter',\n",
    "    # Take only the end of each parameter name for brevity\n",
    "    labels=[i.split('.')[-1] for i in parameter_names],\n",
    "    log_y=False\n",
    ")\n",
    "plot_metric(\n",
    "    [einops.reduce(metric_cache['sum_sq_weights'], 'epoch param -> epoch', 'sum')],\n",
    "    title='Total sum of squared weights',\n",
    "    log_y=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
